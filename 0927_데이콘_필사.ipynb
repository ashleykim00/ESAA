{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0927 데이콘 필사.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwOFxdQeXa+WYShnBomcUc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashleykim00/ESAA/blob/main/0927_%EB%8D%B0%EC%9D%B4%EC%BD%98_%ED%95%84%EC%82%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqN6fhtZgsnZ"
      },
      "source": [
        "# [NLP 언제까지 미룰래? 일단 들어와!!] #4. word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSiYx19Py3q3"
      },
      "source": [
        "# 1. Keras Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TupnekPQsD3L",
        "outputId": "b1005566-7fa4-47e7-fb07-535126161ebc"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzmwqWcKs6dg"
      },
      "source": [
        "import pandas as pd\n",
        "path='/content/gdrive/MyDrive/train.csv'\n",
        "train = pd.read_csv(path)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7UMAVSIswj0",
        "outputId": "d5adaf3f-8299-45b4-8383-87d5d30848dd"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  42331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug67skg3tzPB"
      },
      "source": [
        "from keras.models import Sequential"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwmnzXrJu-AD"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense,Activation,Embedding\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-Df2UMAtorP"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 128, input_length = 100))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sam8aDUfy-pN"
      },
      "source": [
        "# word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05XSwUgbrxIt"
      },
      "source": [
        "import gensim\n",
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary = True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XHv52vNyaS3"
      },
      "source": [
        "tokenizer = Tokenizer() #keras의 vectorizing 함수 호출"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-cevnUcrxDf"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZucNe78rw9s"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = 100))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw9FunoOzCdJ"
      },
      "source": [
        "# glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcGG8AtkwOWQ"
      },
      "source": [
        "2. # load the whole embedding into memory\n",
        "glove = dict()\n",
        "f = open('/content/gdrive/MyDrive/word-embeddings/glove/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZUFEGEtwOTu"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFcJ0V-twORg"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = 100))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvWn69vtzckl"
      },
      "source": [
        "# Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWv1OD1brw4W"
      },
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "FastText = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/word-embeddings/fasttext/fasttext.vec', binary=True, unicode_errors='ignore')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HQmbDdkzebv"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 300)) #300차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in word2vec: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = word2vec[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_mxtrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다."
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5l2T9IC0aK8"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 300,weights = [embedding_matrix], input_length = 100))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIHwJrzS8rwS"
      },
      "source": [
        "# [NLP 언제까지 미룰래? 일단 들어와!!] #5. Modeling(완)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FxTvIzs-W9H",
        "outputId": "74b899de-5c3f-4127-f465-b767e0261286"
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 2.0 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU_qHH9n0aFh",
        "outputId": "63e33dc1-e5c4-4014-fb6a-93e5b4e72b17"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "import tqdm \n",
        "\n",
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in tqdm.tqdm(text_list):\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "\n",
        "train['token'], okt = text_preprocessing(train['text'])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54879/54879 [08:57<00:00, 102.13it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "2A_G5Qdb_5ft",
        "outputId": "95035317-8f4f-4e4e-dd33-fb341b5eebe6"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>He was almost choking. There was so much, so m...</td>\n",
              "      <td>3</td>\n",
              "      <td>[e, was, almost, choking, here, was, so, much,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>“Your sister asked for it, I suppose?”</td>\n",
              "      <td>2</td>\n",
              "      <td>[our, sister, asked, for, it, suppose]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>She was engaged one day as she walked, in per...</td>\n",
              "      <td>1</td>\n",
              "      <td>[he, was, engaged, one, day, as, she, walked, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The captain was in the porch, keeping himself ...</td>\n",
              "      <td>4</td>\n",
              "      <td>[he, captain, was, in, the, porch, keeping, hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
              "      <td>3</td>\n",
              "      <td>[ave, mercy, gentlemen, odin, flung, up, his, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index  ...                                              token\n",
              "0      0  ...  [e, was, almost, choking, here, was, so, much,...\n",
              "1      1  ...             [our, sister, asked, for, it, suppose]\n",
              "2      2  ...  [he, was, engaged, one, day, as, she, walked, ...\n",
              "3      3  ...  [he, captain, was, in, the, porch, keeping, hi...\n",
              "4      4  ...  [ave, mercy, gentlemen, odin, flung, up, his, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS4KGbo0_xos",
        "outputId": "cab5d0d0-0e01-45db-d077-bf1d10ad6ee8"
      },
      "source": [
        "def text2sequence(train_text, max_len=1000):\n",
        "    \n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text)\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len)\n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_y = train['author']\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['token'], max_len = 100)\n",
        "print(train_X.shape, train_y.shape)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size :  36342\n",
            "(54879, 100) (54879,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzernbri_xmd"
      },
      "source": [
        "word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/gdrive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
        "embedding_matrix = np.zeros((vocab_size, 300))\n",
        "\n",
        "for index, word in enumerate(tokenizer.word_index):\n",
        "    if word in word2vec :\n",
        "        embedding_vector = word2vec[word] \n",
        "        embedding_mxtrix[i] = embedding_vector \n",
        "    else:\n",
        "        print(\"word2vec에 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR2R2cwfABsK"
      },
      "source": [
        "def LSTM(vocab_size, max_len=1000):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 300,weights = [embedding_matrx], input_length = max_len)) #임베딩 가중치 적용 코드\n",
        "    model.add(SpatialDropout1D(0.3))\n",
        "    model.add(LSTM(64))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer = regularizers.l2(0.001)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3qdEKYq_xjs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJpZGRyx0aCR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}